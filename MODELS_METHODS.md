# Models & Methods - Architecture & Implementation Details

## Overview

ResoMap implements multiple CNN architectures from `src/models.py` (443 lines), each optimized for different performance/accuracy tradeoffs. All models support variable input resolutions via adaptive pooling.

**Training Status:**
- âœ… **Trained:** simple_cnn, tiny_cnn (10 experiments completed over 2 days)
- ğŸ“‹ **Available:** VGG, ResNet, MobileNet families (ready for future experiments)

**View Results:** https://dagshub.com/Y-R-A-V-R-5/ResoMap/experiments

---

## ğŸ—ï¸ VGG Family

### Architecture Overview
VGG models use **stacked convolutional layers** organized into stages, followed by adaptive global average pooling and fully connected layers.

```
Input (variable resolution)
    â†“
Stages (repeating Conv+ReLU blocks)
    â”œâ”€ Stage 1: 3â†’64 channels
    â”œâ”€ Stage 2: 64â†’128 channels
    â”œâ”€ Stage 3: 128â†’256 channels
    â”œâ”€ Stage 4: 256â†’512 channels
    â””â”€ Stage 5: 512â†’512 channels
    â†“
AdaptiveAvgPool2d (7Ã—7) â† Handles any resolution!
    â†“
Classifier (FC layers + Dropout)
    â†“
Output (num_classes)
```

### Implementation Details

**Class:** `VGG(nn.Module)` in [src/models.py](src/models.py)

**Key Features:**
- Modular stage-based construction from config
- Adaptive average pooling for any resolution
- Configurable FC layer sizes (default: 4096â†’4096â†’num_classes)
- Dropout regularization (default: 0.5)
- ReLU activations throughout

**Variants:**
```python
vgg11:  [1, 1, 2, 2, 2]   # 11 conv layers
vgg13:  [2, 2, 2, 2, 2]   # 13 conv layers
vgg16:  [2, 2, 3, 3, 3]   # 16 conv layers (ImageNet standard)
vgg19:  [2, 2, 3, 4, 4]   # 19 conv layers (deeper)
```

Numbers represent how many Conv layers per stage.

**Parameters:**
- vgg11: ~128M
- vgg13: ~133M
- vgg16: ~138M (most commonly trained)

**Code Reference:**
```python
def forward(self, x):
    # x shape: (batch, 3, H, W) - H and W can be any size
    for stage in self.stages.values():
        x = stage(x)  # Each stage ends with MaxPool
    
    x = self.avgpool(x)  # (batch, 512, 7, 7) for any input
    x = x.view(x.size(0), -1)  # Flatten
    x = self.classifier(x)  # FC layers
    return x
```

**Why Adaptive Pooling Works:**
- After all MaxPool operations, spatial dimensions are reduced
- `AdaptiveAvgPool2d((7,7))` guarantees 7Ã—7 output regardless of input
- This enables fixed FC layer input (512Ã—7Ã—7 = 25,088 features)

**Best For:**
- âœ… Explainability research (simple, interpretable)
- âœ… Understanding layer-wise behavior
- âœ… GPU comparison (baseline architecture)
- âŒ Mobile inference (too many parameters)

---

## ğŸ”— ResNet Family

### Architecture Overview
ResNet uses **residual connections** (skip connections) to train very deep networks without gradient degradation.

```
Input (variable resolution)
    â†“
Initial Conv (7Ã—7, stride=2)
    â†“
Layer1: Multiple Blocks with residuals
Layer2: Multiple Blocks with residuals
Layer3: Multiple Blocks with residuals
Layer4: Multiple Blocks with residuals
    â†“
AdaptiveAvgPool2d (1Ã—1)
    â†“
FC layer (num_features â†’ num_classes)
    â†“
Output
```

### Block Types

**BasicBlock** (ResNet18, ResNet34):
```
Input
  â†“
Conv 3Ã—3 â†’ BatchNorm â†’ ReLU
  â†“
Conv 3Ã—3 â†’ BatchNorm
  â†“
Add with skip connection
  â†“
ReLU
  â†“
Output
```

**Bottleneck** (ResNet50, ResNet101):
```
Input
  â†“
Conv 1Ã—1 (reduce)
  â†“
Conv 3Ã—3 (main)
  â†“
Conv 1Ã—1 (expand)
  â†“
BatchNorm â†’ Add with skip â†’ ReLU
  â†“
Output
```

### Implementation Details

**Class:** `ResNet(nn.Module)` in [src/models.py](src/models.py)

**Key Features:**
- Block repetition counts configurable per layer
- Bottleneck blocks for depth (ResNet50+)
- Batch normalization throughout
- Identity skip connections (straight path)
- Projected skip connections when spatial dims change
- Stride-2 in first block of layer 2-4 (downsampling)

**Variants:**
```python
resnet18:  [2, 2, 2, 2] blocks per layer + BasicBlock
resnet34:  [3, 4, 6, 3] blocks per layer + BasicBlock
resnet50:  [3, 4, 6, 3] blocks per layer + Bottleneck
resnet101: [3, 4, 23, 3] blocks per layer + Bottleneck
```

**Parameters:**
- resnet18: ~11M (lightweight!)
- resnet34: ~21M
- resnet50: ~25M
- resnet101: ~44M

**Code Reference:**
```python
class Bottleneck(nn.Module):
    def forward(self, x):
        identity = x
        
        out = self.conv1(x)  # 1Ã—1 reduce
        out = self.bn1(out)
        out = F.relu(out)
        
        out = self.conv2(out)  # 3Ã—3 main
        out = self.bn2(out)
        out = F.relu(out)
        
        out = self.conv3(out)  # 1Ã—1 expand
        out = self.bn3(out)
        
        out += identity  # Skip connection!
        out = F.relu(out)
        return out
```

**Why Skip Connections Matter:**
- Gradients can flow directly through the skip path
- Allows training very deep networks (101+ layers)
- Each block learns residual (difference) not absolute mapping
- Identity initialization: early training benefits from identity path

**Best For:**
- âœ… Accuracy vs depth analysis
- âœ… Computational efficiency comparison
- âœ… Transfer learning (great pre-trained models available)
- âœ… Mobile deployment (resnet18 is very efficient)
- âœ… Balanced accuracy/speed tradeoff

---

## ğŸ“± MobileNet Family

### Architecture Overview
MobileNet uses **depthwise separable convolutions** to achieve high accuracy with minimal parameters - designed for mobile/edge devices.

```
Input (variable resolution)
    â†“
Conv 3Ã—3 (32 filters)
    â†“
MobileBlock 1: Depthwise + Pointwise (expansion=1)
MobileBlock 2: Depthwise + Pointwise (expansion=6)
... (multiple blocks with different configs)
    â†“
AdaptiveAvgPool (1Ã—1)
    â†“
FC (1000 â†’ num_classes)
    â†“
Output
```

### Depthwise Separable Convolution

**Standard Convolution:**
- Input: (batch, in_channels, H, W)
- Kernel: (out_channels, in_channels, 3, 3)
- Computation: in_channels Ã— H Ã— W Ã— out_channels Ã— 9 operations

**Depthwise Separable:**
1. **Depthwise:** (in_channels, 1, 3, 3) - one filter per channel
2. **Pointwise:** (out_channels, in_channels, 1, 1) - cross-channel mixing

**Benefit:** ~8-9x fewer operations!

### MobileNetV2: Inverted Residual

```
Input (expansion=6 for middle blocks)
  â†“
1Ã—1 Conv (expand by 6x)
  â†“
Depthwise Conv 3Ã—3 (ReLU6)
  â†“
1Ã—1 Conv (project back)
  â†“
Skip connection (only if stride=1)
  â†“
Output
```

Why "inverted"? Traditional ResNet: wideâ†’narrowâ†’wide. MobileNet: narrowâ†’wideâ†’narrow.

### Implementation Details

**Class:** `MobileNetV2(nn.Module)` in [src/models.py](src/models.py)

**Key Features:**
- Configurable expansion factor (default=6)
- Width multiplier (default=1.0, can reduce to 0.75 for smaller models)
- ReLU6 activations
- Batch normalization throughout
- Stride control for spatial downsampling

**MobileNetV3 Additions:**
- Squeeze-and-Excitation (SE) blocks for channel attention
- Hard Swish activation (more efficient)
- More efficient block design

**Parameters:**
- mobilenet_v2 (width=1.0): ~3.5M
- mobilenet_v2_small (width=0.75): ~2.2M
- mobilenet_v3_small: ~2.5M
- mobilenet_v3_large: ~5.4M

**Width Multiplier Effect:**
```
width=1.0:    all_channels Ã— 1.0  â†’ full model
width=0.75:   all_channels Ã— 0.75 â†’ 50% parameters
width=0.5:    all_channels Ã— 0.5  â†’ 25% parameters
```

**Best For:**
- âœ… Mobile/edge device deployment
- âœ… Efficiency vs accuracy analysis
- âœ… Finding smallest model for target accuracy
- âœ… Latency-critical applications
- âœ… Memory-constrained scenarios

---

## ğŸ¯ Custom CNNs

### SimpleCNN

Minimal architecture for quick experimentation and debugging:

```
Input (variable resolution)
  â†“
Conv 3Ã—3 (3â†’32) + ReLU + MaxPool 2Ã—2
  â†“
Conv 3Ã—3 (32â†’64) + ReLU + MaxPool 2Ã—2
  â†“
Conv 3Ã—3 (64â†’128) + ReLU + MaxPool 2Ã—2
  â†“
AdaptiveAvgPool (4Ã—4)
  â†“
FC (128Ã—16 â†’ 128) + ReLU + Dropout
  â†“
FC (128 â†’ num_classes)
  â†“
Output
```

**Parameters:** <1M
**Use Cases:**
- âœ… Quick debugging
- âœ… Testing pipeline functionality
- âœ… Small dataset experiments

### TinyCNN

Even smaller baseline:
```
Input
  â†“
Conv 3Ã—3 (3â†’16) + ReLU
  â†“
MaxPool 2Ã—2
  â†“
Conv 3Ã—3 (16â†’32) + ReLU
  â†“
AdaptiveAvgPool (2Ã—2)
  â†“
FC (32Ã—4 â†’ 10)
  â†“
Output
```

**Parameters:** <0.5M

---

## ğŸ”„ Building Models from Config

### Method 1: From config.yaml

```python
from src.models import build_model
from src.utils import load_config

config = load_config('configs/models.yaml')
model_cfg = config['models']['vgg11']

model = build_model(model_cfg, num_classes=7)
# Returns: VGG model for skin lesion classification (7 classes)
```

### Method 2: Using load_model_from_config

```python
from src.models import load_model_from_config

model = load_model_from_config('resnet50', num_classes=10)
# Automatically loads architecture from configs/models.yaml
```

### Method 3: Direct initialization

```python
from src.models import ResNet, Bottleneck

model = ResNet(
    block=Bottleneck,
    block_counts=[3, 4, 6, 3],  # ResNet50 config
    num_classes=7
)
```

---

## ğŸ“Š Model Comparison Table

| Model | Params | Size | Latency | Accuracy* | Explainability | Use Case |
|-------|--------|------|---------|-----------|---|---|
| simple_cnn | <1M | <5MB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | â­â­â­â­â­ | Debugging |
| tiny_cnn | <0.5M | <2MB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | â­â­â­â­â­ | Baseline |
| mobilenet_v2_small | 2.2M | 10MB | âš¡âš¡âš¡âš¡ | â­â­â­ | â­â­â­ | Mobile |
| mobilenet_v2 | 3.5M | 14MB | âš¡âš¡âš¡âš¡ | â­â­â­ | â­â­â­ | Mobile |
| resnet18 | 11M | 45MB | âš¡âš¡âš¡ | â­â­â­â­ | â­â­â­â­ | Balanced |
| resnet50 | 25M | 100MB | âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ | Production |
| vgg16 | 138M | 500MB | âš¡ | â­â­â­â­â­ | â­â­â­â­â­ | Research |

*Approximate accuracy on ImageNet (100 class)

---

## ğŸ› ï¸ Advanced Features

### Adaptive Pooling for Variable Resolutions

All models use `AdaptiveAvgPool2d()` or `AdaptiveMaxPool2d()` instead of fixed pooling:

```python
# Standard approach (fixed size input)
self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)
# Works only for 224Ã—224, breaks for other sizes

# Adaptive approach (any size input)
self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))
# Works for 64Ã—64, 224Ã—224, 512Ã—512, anything!
```

**How it works:**
- Calculates stride/kernel dynamically: stride = input_size / output_size
- For 224Ã—224 input â†’ stride â‰ˆ 32, kernel â‰ˆ 32
- For 512Ã—512 input â†’ stride â‰ˆ 73, kernel â‰ˆ 73
- Always outputs exactly 7Ã—7 (or specified size)

### Weight Initialization

All models use proper weight initialization:

```python
for m in model.modules():
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.constant_(m.bias, 0)
```

- Kaiming (He) for convolutional layers
- Normal distribution for FC layers
- BatchNorm initialized to identity

---

## ğŸ“ˆ Architecture Selection Guide

**Choose VGG if:**
- Studying layer-wise behavior
- Need maximum interpretability
- Have sufficient GPU memory
- Analyzing Grad-CAM visualizations

**Choose ResNet if:**
- Want best accuracy/parameter tradeoff
- Training on limited GPU memory
- Need transfer learning models
- Production deployment planned

**Choose MobileNet if:**
- Deploying to mobile/edge devices
- Optimizing for inference speed
- Memory is critical constraint
- Need real-time performance

**Choose Custom CNNs if:**
- Debugging the pipeline
- Quick experimentation
- Establishing baseline
- Research into architecture basics

---

**Next:** [TRAINING_EXECUTION.md](TRAINING_EXECUTION.md) - How to train these models  
**Back:** [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) - Project overview
